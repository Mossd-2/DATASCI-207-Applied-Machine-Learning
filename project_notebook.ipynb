{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ca8xNDSodnJt"
      },
      "source": [
        "## 207 -Applied Machine Learning Project: Predicting Attrition of an Online Store Site\n",
        "\n",
        "#### Authors:\n",
        "\n",
        "Diego Moss\\\n",
        "Sammy Cayo\\\n",
        "Conor Huh\\\n",
        "Roz Huang\\\n",
        "Jasmine Lau\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jq4Va677dnJu"
      },
      "source": [
        "## Loading Data from BigQuery and Binding Rows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1-bcyeLEdnJu"
      },
      "outputs": [],
      "source": [
        "## code block for initial data loading\n",
        "\n",
        "from google.colab import auth\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from google.cloud import bigquery\n",
        "from datetime import datetime, timedelta\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "\n",
        "\n",
        "def query_data():\n",
        "  sys.path.append('/content/src/bq-helper')\n",
        "  from bq_helper import BigQueryHelper\n",
        "  !pip install -e git+https://github.com/ConorHuh/BigQuery_Helper#egg=bq_helper\n",
        "  auth.authenticate_user()\n",
        "  project_id = \"final-project-430217\" # change this to correct project ID\n",
        "  client = bigquery.Client(project=project_id)\n",
        "\n",
        "  google_analytics = BigQueryHelper(active_project=\"bigquery-public-data\",\n",
        "                                    dataset_name=\"data:google_analytics_sample\",\n",
        "                                    project_id=project_id)\n",
        "\n",
        "  table_names = []\n",
        "  all_data = []\n",
        "  all_hits_data = []\n",
        "\n",
        "  start_date = datetime(2016, 8, 1)\n",
        "  end_date = datetime(2017, 8, 1)\n",
        "  current_date = start_date\n",
        "\n",
        "  while current_date <= end_date:\n",
        "      table_name = f\"ga_sessions_{current_date.strftime('%Y%m%d')}\"\n",
        "\n",
        "      query = f\"\"\"\n",
        "      SELECT *\n",
        "      FROM `bigquery-public-data.google_analytics_sample.{table_name}`\n",
        "      \"\"\"\n",
        "      table_names.append(table_name)\n",
        "      df = google_analytics.query_to_pandas(query)\n",
        "\n",
        "      totals = pd.json_normalize(df['totals'])\n",
        "      totals.columns = ['total_' + col for col in totals.columns]\n",
        "      df = pd.concat([df.drop(columns=['totals']), totals], axis=1)\n",
        "\n",
        "      source = pd.json_normalize(df['trafficSource'])\n",
        "      source.columns = ['trafficSource_' + col for col in source.columns]\n",
        "      df = pd.concat([df.drop(columns=['trafficSource']), source], axis=1)\n",
        "\n",
        "      device = pd.json_normalize(df['device'])\n",
        "      device.columns = ['device_' + col for col in device.columns]\n",
        "      df = pd.concat([df.drop(columns=['device']), device], axis=1)\n",
        "\n",
        "      network = pd.json_normalize(df['geoNetwork'])\n",
        "      network.columns = ['geoNetwork_' + col for col in network.columns]\n",
        "      df = pd.concat([df.drop(columns=['geoNetwork']), network], axis=1)\n",
        "\n",
        "      custom_dimensions = pd.json_normalize(df.explode('customDimensions')['customDimensions'])\n",
        "      custom_dimensions.columns = ['customDimensions_' + col for col in custom_dimensions.columns]\n",
        "      df = pd.concat([df.drop(columns=['customDimensions']), custom_dimensions], axis=1)\n",
        "\n",
        "      # df.to_csv(f'{table_name}.csv')\n",
        "      all_data.append(df)\n",
        "\n",
        "      hits_df = pd.json_normalize(df.explode('hits')['hits'])\n",
        "      repeated_visit_ids = pd.DataFrame(np.repeat(df['visitId'].values, df['hits'].apply(len)), columns=['visitID'])\n",
        "      hits_df = pd.concat([repeated_visit_ids, hits_df], axis=1)\n",
        "      # hits_df.to_csv(f'{table_name}_hits.csv')\n",
        "      all_hits_data.append(hits_df)\n",
        "\n",
        "      current_date += timedelta(days=1)\n",
        "\n",
        "  final_df = pd.concat(all_data, ignore_index=True)\n",
        "  final_hits_df = pd.concat(all_hits_data, ignore_index=True)\n",
        "\n",
        "  final_df.to_csv('all_data.csv', index=False)\n",
        "  final_hits_df.to_csv('all_hits_data.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iugKmsnmyL95"
      },
      "source": [
        "# Load Data from Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I-SEOSqOyLTu",
        "outputId": "c137162b-dac3-47ef-f8f8-51c468672bfd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Connect to GDrive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K4ViHhMuy--C",
        "outputId": "7e3d1716-1a45-424b-bd97-8553c9bf03d1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-3-c114907bf6cc>:4: DtypeWarning: Columns (6,30) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(path)\n"
          ]
        }
      ],
      "source": [
        "DF_DATA_PATH = '/content/drive/MyDrive/google_analytics_sample/all_data.csv'\n",
        "HITS_DATA_PATH = '/content/drive/MyDrive/google_analytics_sample/all_hits_data.csv'\n",
        "def load_final_df(path):\n",
        "    df = pd.read_csv(path)\n",
        "    return df\n",
        "df = load_final_df(DF_DATA_PATH)\n",
        "#hits_df = load_final_df(HITS_DATA_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "khLAfUW8LVWt"
      },
      "outputs": [],
      "source": [
        "# General Exploration\n",
        "\n",
        "unique_visits = hits_df['visitID'].unique() # these are not unique, but are unique to each user.\n",
        "\n",
        "# Given visitId's are not unique to a given date, we need to adapt the data.\n",
        "# grouped_counts = df.groupby(\"visitId\")['fullVisitorId'].value_counts() # multiple visitID's\n",
        "# filtered_counts = grouped_counts[grouped_counts > 1]\n",
        "# print(filtered_counts.to_string())\n",
        "\n",
        "#print(df[df['visitId'] == 1470120798])\n",
        "print(hits_df.columns.to_list())\n",
        "\n",
        "unique_visitors = df['fullVisitorId'].unique()\n",
        "\n",
        "# Ensuring that we have a truly unique of hits df (based on visitID & visitorID)\n",
        "# I cannot do this work on Google Colab. Not enough RAM\n",
        "\n",
        "# hits_df_temp = pd.json_normalize(df.explode('hits')['hits'])\n",
        "# df['combinedId'] = df['visitId'].astype(str) + '_' + df['fullVisitorId'].astype(str)\n",
        "# repeated_combined_ids = pd.DataFrame(np.repeat(df['combinedId'].values, df['hits'].apply(len)), columns=['combinedId'])\n",
        "# unique_hits_df = pd.concat([repeated_combined_ids, hits_df_temp], axis=1)\n",
        "\n",
        "ecommerce_counts = hits_df.groupby('visitID')['eCommerceAction.action_type'].value_counts().unstack(fill_value=0) # this will work when hits data is unique\n",
        "\n",
        "len(unique_visitors) #718,161\n",
        "\n",
        "cols = df.columns\n",
        "print(cols)\n",
        "print(ecommerce_counts[0])\n",
        "\n",
        "# meet w/ group on these, given challenges in linking visitID to visitorID & date\n",
        "hit_features = [\n",
        "    \"num_product_list_views\", #1\n",
        "    \"num_product_detail_views\", #2\n",
        "    \"num_add_to_cart\", #3\n",
        "    \"num_remove_from_cart\", #4\n",
        "    \"checkout\", #5\n",
        "    \"purchase\", #6\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XMS9KLs9QlgM",
        "outputId": "95d04e17-28fa-4fca-d17e-f41f7efa39e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(718161, 20, 366)\n",
            "[[[  1.   0.   0. ...   0.   0.   0.]\n",
            "  [  1.   0.   0. ...   0.   0.   0.]\n",
            "  [  1.   0.   0. ...   0.   0.   0.]\n",
            "  ...\n",
            "  [375.   0.   0. ...   0.   0.   0.]\n",
            "  [ 93.   0.   0. ...   0.   0.   0.]\n",
            "  [648.   0.   0. ...   0.   0.   0.]]\n",
            "\n",
            " [[  0.   1.   0. ...   0.   0.   0.]\n",
            "  [  0.  10.   0. ...   0.   0.   0.]\n",
            "  [  0.   8.   0. ...   0.   0.   0.]\n",
            "  ...\n",
            "  [  0. 375.   0. ...   0.   0.   0.]\n",
            "  [  0.  93.   0. ...   0.   0.   0.]\n",
            "  [  0. 648.   0. ...   0.   0.   0.]]\n",
            "\n",
            " [[  0.   0.   1. ...   0.   0.   0.]\n",
            "  [  0.   0.  11. ...   0.   0.   0.]\n",
            "  [  0.   0.   8. ...   0.   0.   0.]\n",
            "  ...\n",
            "  [  0.   0. 375. ...   0.   0.   0.]\n",
            "  [  0.   0.  93. ...   0.   0.   0.]\n",
            "  [  0.   0. 648. ...   0.   0.   0.]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[  0.   0.   0. ...   0.   0.   0.]\n",
            "  [  0.   0.   0. ...   0.   0.   0.]\n",
            "  [  0.   0.   0. ...   0.   0.   0.]\n",
            "  ...\n",
            "  [  0.   0.   0. ...   0.   0.   0.]\n",
            "  [  0.   0.   0. ...   0.   0.   0.]\n",
            "  [  0.   0.   0. ...   0.   0.   0.]]\n",
            "\n",
            " [[  0.   0.   0. ...   0.   0.   0.]\n",
            "  [  0.   0.   0. ...   0.   0.   0.]\n",
            "  [  0.   0.   0. ...   0.   0.   0.]\n",
            "  ...\n",
            "  [  0.   0.   0. ...   0.   0.   0.]\n",
            "  [  0.   0.   0. ...   0.   0.   0.]\n",
            "  [  0.   0.   0. ...   0.   0.   0.]]\n",
            "\n",
            " [[  0.   0.   0. ...   0.   0.   0.]\n",
            "  [  0.   0.   0. ...   0.   0.   0.]\n",
            "  [  0.   0.   0. ...   0.   0.   0.]\n",
            "  ...\n",
            "  [  0.   0.   0. ...   0.   0.   0.]\n",
            "  [  0.   0.   0. ...   0.   0.   0.]\n",
            "  [  0.   0.   0. ...   0.   0.   0.]]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "# TZ in data: GMT-7\n",
        "\n",
        "df = df.sort_values(by=['fullVisitorId', 'date'])\n",
        "\n",
        "feature_columns = [\n",
        "    \"fullVisitorId\",\n",
        "    \"visitNumber\",\n",
        "    \"date\",\n",
        "    \"total_hits\",\n",
        "    \"total_pageviews\",\n",
        "    \"total_screenviews\",\n",
        "    \"total_sessionQualityDim\",\n",
        "    \"total_timeOnSite\",\n",
        "    \"total_totalTransactionRevenue\",\n",
        "    \"total_transactions\",\n",
        "    \"trafficSource_source\",\n",
        "    \"socialEngagementType\",\n",
        "    \"channelGrouping\",\n",
        "    \"device_browser\",\n",
        "    \"device_operatingSystem\",\n",
        "    \"device_deviceCategory\",\n",
        "    \"geoNetwork_continent\",\n",
        "    \"geoNetwork_subContinent\",\n",
        "    \"geoNetwork_country\",\n",
        "    \"geoNetwork_region\",\n",
        "    \"geoNetwork_metro\",\n",
        "    \"geoNetwork_city\",\n",
        "]\n",
        "\n",
        "df = df[feature_columns]\n",
        "\n",
        "numeric_columns = [\n",
        "    \"visitNumber\",\n",
        "    \"total_hits\",\n",
        "    \"total_pageviews\",\n",
        "    \"total_screenviews\",\n",
        "    \"total_sessionQualityDim\",\n",
        "    \"total_timeOnSite\",\n",
        "    \"total_totalTransactionRevenue\",\n",
        "    \"total_transactions\"\n",
        "]\n",
        "\n",
        "non_numeric_columns = [\n",
        "    \"trafficSource_source\",\n",
        "    \"socialEngagementType\",\n",
        "    \"channelGrouping\",\n",
        "    \"device_browser\",\n",
        "    \"device_operatingSystem\",\n",
        "    \"device_deviceCategory\",\n",
        "    \"geoNetwork_continent\",\n",
        "    \"geoNetwork_subContinent\",\n",
        "    \"geoNetwork_country\",\n",
        "    \"geoNetwork_region\",\n",
        "    \"geoNetwork_metro\",\n",
        "    \"geoNetwork_city\",\n",
        "]\n",
        "\n",
        "# Encode non-numeric columns\n",
        "label_encoders = {}\n",
        "for col in non_numeric_columns:\n",
        "    le = LabelEncoder()\n",
        "    df[col] = le.fit_transform(df[col].astype(str))\n",
        "    label_encoders[col] = le\n",
        "\n",
        "unique_users = df['fullVisitorId'].unique()\n",
        "unique_dates = df['date'].unique()\n",
        "\n",
        "# - 2 because fullVisitorId and date are not features\n",
        "array_shape = (len(unique_users), len(feature_columns) - 2, len(unique_dates))\n",
        "array_3d = np.zeros(array_shape, dtype=np.float32)\n",
        "\n",
        "# Create a dictionary to map fullVisitorId and date to index\n",
        "user_index = {user: i for i, user in enumerate(unique_users)}\n",
        "date_index = {date: j for j, date in enumerate(unique_dates)}\n",
        "\n",
        "\n",
        "for row in df.itertuples(index=False):\n",
        "    user_idx = user_index[row.fullVisitorId]\n",
        "    date_idx = date_index[row.date]\n",
        "\n",
        "    # Numeric values\n",
        "    array_3d[user_idx, 0:len(numeric_columns), date_idx] = [getattr(row, col) for col in numeric_columns]\n",
        "\n",
        "    # Non-numeric values\n",
        "    array_3d[user_idx, len(numeric_columns):, date_idx] = [getattr(row, col) for col in non_numeric_columns]\n",
        "\n",
        "print(array_3d.shape)\n",
        "print(array_3d)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hA-sdoogdnJv"
      },
      "source": [
        "## Initial Data Pre-Processing and Feature Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HngzXs1SdnJv"
      },
      "outputs": [],
      "source": [
        "## code block for preprocessing\n",
        "\n",
        "## after binding rows we will need to extract the data from the columns with multiple data and put into their own columns\n",
        "\n",
        "## should code categorical variables into one-encoding\n",
        "\n",
        "## we need to manipulate and aggregate each user's data for each month for all the manipulated features in our list\n",
        "\n",
        "## we should take a look at how bad we're missing data and decide for imputation or removal\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## shaping the data in the right way for the LSTM model (3 dimensions)\n",
        "\n",
        "def create_sequences(df, sequence_length):\n",
        "    sequences = []\n",
        "    targets = []\n",
        "    user_ids = df['user_id'].unique()\n",
        "\n",
        "    for user_id in user_ids:\n",
        "        user_data = df[df['user_id'] == user_id]\n",
        "        user_data = user_data.sort_values(by='month')  # Ensure data is sorted by time\n",
        "        for i in range(len(user_data) - sequence_length):\n",
        "            seq = user_data.iloc[i:i+sequence_length].drop(columns=['user_id', 'days_until_next_visit']).values # target can change depending on our attrition statistics\n",
        "            target = user_data.iloc[i+sequence_length]['days_until_next_visit'] # target can change depending on our attrition statistics\n",
        "            sequences.append(seq)\n",
        "            targets.append(target)\n",
        "\n",
        "    return sequences, np.array(targets)\n",
        "\n",
        "# 12 months\n",
        "sequence_length = 12\n",
        "\n",
        "# creating our sequences and targets for putting into X and y objects\n",
        "data_sequences, targets = create_sequences(df, sequence_length)\n",
        "\n",
        "# Pad sequences to ensure all have the same length\n",
        "data_sequences_padded = pad_sequences(data_sequences, maxlen=sequence_length, padding='post', dtype='float32')\n",
        "\n",
        "# Convert to arrays and saving as X and y\n",
        "X = np.array(data_sequences_padded)\n",
        "y = np.array(targets)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apRKD8H-dnJv"
      },
      "source": [
        "## Splitting Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zBYhz2iwdnJv"
      },
      "outputs": [],
      "source": [
        "## splitting data code block\n",
        "\n",
        "## need to see distribution of target variable and distribution of months in the data. If unequal need to sample so that the training set has roughly equal targets and months\n",
        "\n",
        "## may need to manually split the data, make sure we are splitting by user and not by month\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTG78KMadnJv"
      },
      "source": [
        "## More Data Processing and EDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xeVYtE0hdnJv"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "## EDA - visualizing distributions and correlations\n",
        "\n",
        "\n",
        "    # histograms for each feature\n",
        "\n",
        "\n",
        "    # correlations matrix/plot/heatmap\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## standardizing features based on the training distributions\n",
        "\n",
        "\n",
        "\n",
        "# defining lists of indices for the numerical and categorical features and telling it to ignore one-hot encoded variables\n",
        "\n",
        "# replace with indices for all the numeric columns to be standardized (need to calculate which indices after flattening from 3d to 2d)\n",
        "numerical_features_indices = [None,None]\n",
        "\n",
        "# replace with indices for all the one-hot encoded columns to be ignored (need to calculate which indices after flattening from 3d to 2d)\n",
        "one_hot_encoded_indices = [None,None]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# making a preprocessor to standardize and ignore features (also this is were we can impute data if we'd like)\n",
        "numerical_transformer = Pipeline(steps=[ # this says to do the below manipulations to each specified column\n",
        "    ('imputer', SimpleImputer(strategy='median')),  # This is the handling of missing values, placeholder for now\n",
        "    ('scaler', StandardScaler()) # this standardizes\n",
        "])\n",
        "\n",
        "# Combine transformers into a ColumnTransformer\n",
        "preprocessor = ColumnTransformer( # this takes the pipeline from before and maps which columns to apply the numerical transformations\n",
        "    transformers=[\n",
        "        ('num', numerical_transformer, numerical_features_indices),\n",
        "        ('cat', 'passthrough', one_hot_encoded_indices)\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# fitting on 2d reshaped X_train because the scaler won't work on 3 dimensions, then returning back into 3 dimensions\n",
        "X_train = preprocessor.fit_transform(X_train.reshape(X_train.shape[0]*X_train.shape[1], X_train.shape[-1])).reshape(X_train.shape)\n",
        "# doing same for test set\n",
        "X_test = preprocessor.transform(X_test.reshape(X_train.shape[0]*X_train.shape[1], X_test.shape[-1])).reshape(X_test.shape)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JiugqgJidnJw"
      },
      "source": [
        "## Model Fitting and Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TBW3bHRMdnJw"
      },
      "outputs": [],
      "source": [
        "from keras.models import Model, Sequential\n",
        "from keras.layers import Input, LSTM, Dense, TimeDistributed\n",
        "\n",
        "\n",
        "# may need to turn this into a function so we can use in hyperparameter tuning\n",
        "\n",
        "\n",
        "## Defining Model\n",
        "\n",
        "# need to separate the stable features from the dynamic features fix this once we know which features\n",
        "X_train_dynamic = None\n",
        "X_train_stable = None\n",
        "\n",
        "\n",
        "# Input for dynamic features\n",
        "dynamic_input = Input(shape=(X_train_dynamic.shape[1], X_train_dynamic.shape[2]))\n",
        "x = LSTM(units=50, return_sequences=True)(dynamic_input) # the parantheses here are telling the function to apply only to the dynamic features\n",
        "\n",
        "\n",
        "# Input for stable features\n",
        "stable_input = Input(shape=(X_train_stable.shape[1],))\n",
        "stable_repeated = Dense(X_train_dynamic.shape[1])(stable_input)\n",
        "stable_repeated = RepeatVector(X_train_dynamic.shape[1])(stable_repeated)\n",
        "stable_repeated = TimeDistributed(Dense(50))(stable_repeated)\n",
        "\n",
        "\n",
        "# Concatenate LSTM output with stable features\n",
        "x = Concatenate()([x, stable_input])\n",
        "\n",
        "# TimeDistributed Dense layer to get prediction for each timestep\n",
        "output = TimeDistributed(Dense(1, activation='sigmoid'))(x)\n",
        "\n",
        "# Define the model\n",
        "model = Model(inputs=[dynamic_input, stable_input], outputs=output)\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics = ['accuracy', 'precision'])\n",
        "# should look as precision because we care more about correctly catching attrition over falsely claiming someone will attrit\n",
        "\n",
        "# Train the model\n",
        "model.fit([X_train_dynamic, X_train_stable], y_train_scaled, epochs=10, validation_split=0.2)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ak9_h0cDdnJw"
      },
      "source": [
        "## Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "50FXbArzdnJw"
      },
      "outputs": [],
      "source": [
        "## placeholder\n",
        "\n",
        "## we need to see how long it takes to fit one epoch, then do some math to see how many sets of hyperparameters we can test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PlGsizINdnJw"
      },
      "source": [
        "## Evaluation Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Y3SeLO-dnJw"
      },
      "outputs": [],
      "source": [
        "## placeholder"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "history_visible": true,
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
