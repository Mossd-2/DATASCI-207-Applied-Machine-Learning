{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 207 -Applied Machine Learning Project: Predicting Attrition of an Online Store Site\n",
    "\n",
    "#### Authors:\n",
    "\n",
    "Diego Moss\n",
    "Sammy Cayo\n",
    "Conor Huh\n",
    "Roz Huang\n",
    "Jasmine Lau\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data from BigQuery and Binding Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## code block for initial data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Data Pre-Processing and Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## code block for preprocessing\n",
    "\n",
    "## after binding rows we will need to extract the data from the columns with multiple data and put into their own columns\n",
    "\n",
    "## we need to manipulate and aggregate each user's data for each month for all the manipulated features in our list\n",
    "\n",
    "## we should take a look at how bad we're missing data and decide for imputation or removal\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## shaping the data in the right way for the LSTM model (3 dimensions)\n",
    "\n",
    "def create_sequences(df, sequence_length):\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    user_ids = df['user_id'].unique()\n",
    "    \n",
    "    for user_id in user_ids:\n",
    "        user_data = df[df['user_id'] == user_id]\n",
    "        user_data = user_data.sort_values(by='month')  # Ensure data is sorted by time\n",
    "        for i in range(len(user_data) - sequence_length):\n",
    "            seq = user_data.iloc[i:i+sequence_length].drop(columns=['user_id', 'days_until_next_visit']).values # target can change depending on our attrition statistics\n",
    "            target = user_data.iloc[i+sequence_length]['days_until_next_visit'] # target can change depending on our attrition statistics\n",
    "            sequences.append(seq)\n",
    "            targets.append(target)\n",
    "    \n",
    "    return sequences, np.array(targets)\n",
    "\n",
    "# 12 months\n",
    "sequence_length = 12\n",
    "\n",
    "# creating our sequences and targets for putting into X and y objects\n",
    "data_sequences, targets = create_sequences(df, sequence_length)\n",
    "\n",
    "# Pad sequences to ensure all have the same length\n",
    "data_sequences_padded = pad_sequences(data_sequences, maxlen=sequence_length, padding='post', dtype='float32')\n",
    "\n",
    "# Convert to arrays and saving as X and y\n",
    "X = np.array(data_sequences_padded)\n",
    "y = np.array(targets)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## splitting data code block\n",
    "\n",
    "## need to see distribution of target variable and distribution of months in the data. If unequal need to sample so that the training set has roughly equal targets and months\n",
    "\n",
    "## may need to manually split the data, make sure we are splitting by user and not by rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Data Processing and EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## EDA - visualizing distributions and correlations\n",
    "\n",
    "\n",
    "\n",
    "## standardizing features based on the training distributions\n",
    "\n",
    "# Standardize the features (optional)\n",
    "scaler = StandardScaler()\n",
    "# fitting on 2d reshaped X_train because the scaler won't work on 3 dimensions, then returning back into 3 dimensions\n",
    "X_train = scaler.fit_transform(X_train.reshape(X_train.shape[0]*X_train.shape[1], X_train.shape[-1])).reshape(X_train.shape) \n",
    "# doing same for test set\n",
    "X_test = scaler.transform(X_test.reshape(X_train.shape[0]*X_train.shape[1], X_test.shape[-1])).reshape(X_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Fitting and Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Placeholder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## placeholder\n",
    "\n",
    "## we need to see how long it takes to fit one epoch, then do some math to see how many sets of hyperparameters we can test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## placeholder"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
