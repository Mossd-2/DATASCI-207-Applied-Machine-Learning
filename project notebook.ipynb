{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 207 -Applied Machine Learning Project: Predicting Attrition of an Online Store Site\n",
    "\n",
    "#### Authors:\n",
    "\n",
    "Diego Moss\n",
    "Sammy Cayo\n",
    "Conor Huh\n",
    "Roz Huang\n",
    "Jasmine Lau\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data from BigQuery and Binding Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## code block for initial data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Data Pre-Processing and Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## code block for preprocessing\n",
    "\n",
    "## after binding rows we will need to extract the data from the columns with multiple data and put into their own columns\n",
    "\n",
    "## should code categorical variables into one-encoding\n",
    "\n",
    "## we need to manipulate and aggregate each user's data for each month for all the manipulated features in our list\n",
    "\n",
    "## we should take a look at how bad we're missing data and decide for imputation or removal\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## shaping the data in the right way for the LSTM model (3 dimensions)\n",
    "\n",
    "def create_sequences(df, sequence_length):\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    user_ids = df['user_id'].unique()\n",
    "    \n",
    "    for user_id in user_ids:\n",
    "        user_data = df[df['user_id'] == user_id]\n",
    "        user_data = user_data.sort_values(by='month')  # Ensure data is sorted by time\n",
    "        for i in range(len(user_data) - sequence_length):\n",
    "            seq = user_data.iloc[i:i+sequence_length].drop(columns=['user_id', 'days_until_next_visit']).values # target can change depending on our attrition statistics\n",
    "            target = user_data.iloc[i+sequence_length]['days_until_next_visit'] # target can change depending on our attrition statistics\n",
    "            sequences.append(seq)\n",
    "            targets.append(target)\n",
    "    \n",
    "    return sequences, np.array(targets)\n",
    "\n",
    "# 12 months\n",
    "sequence_length = 12\n",
    "\n",
    "# creating our sequences and targets for putting into X and y objects\n",
    "data_sequences, targets = create_sequences(df, sequence_length)\n",
    "\n",
    "# Pad sequences to ensure all have the same length\n",
    "data_sequences_padded = pad_sequences(data_sequences, maxlen=sequence_length, padding='post', dtype='float32')\n",
    "\n",
    "# Convert to arrays and saving as X and y\n",
    "X = np.array(data_sequences_padded)\n",
    "y = np.array(targets)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## splitting data code block\n",
    "\n",
    "## need to see distribution of target variable and distribution of months in the data. If unequal need to sample so that the training set has roughly equal targets and months\n",
    "\n",
    "## may need to manually split the data, make sure we are splitting by user and not by month\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Data Processing and EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "## EDA - visualizing distributions and correlations\n",
    "\n",
    "\n",
    "    # histograms for each feature\n",
    "\n",
    "\n",
    "    # correlations matrix/plot/heatmap \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## standardizing features based on the training distributions\n",
    "\n",
    "\n",
    "\n",
    "# defining lists of indices for the numerical and categorical features and telling it to ignore one-hot encoded variables\n",
    "\n",
    "# replace with indices for all the numeric columns to be standardized (need to calculate which indices after flattening from 3d to 2d)\n",
    "numerical_features_indices = [None,None] \n",
    "\n",
    "# replace with indices for all the one-hot encoded columns to be ignored (need to calculate which indices after flattening from 3d to 2d)\n",
    "one_hot_encoded_indices = [None,None] \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# making a preprocessor to standardize and ignore features (also this is were we can impute data if we'd like) \n",
    "numerical_transformer = Pipeline(steps=[ # this says to do the below manipulations to each specified column\n",
    "    ('imputer', SimpleImputer(strategy='median')),  # This is the handling of missing values, placeholder for now\n",
    "    ('scaler', StandardScaler()) # this standardizes\n",
    "])\n",
    "\n",
    "# Combine transformers into a ColumnTransformer\n",
    "preprocessor = ColumnTransformer( # this takes the pipeline from before and maps which columns to apply the numerical transformations\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features_indices),\n",
    "        ('cat', 'passthrough', one_hot_encoded_indices)\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# fitting on 2d reshaped X_train because the scaler won't work on 3 dimensions, then returning back into 3 dimensions\n",
    "X_train = preprocessor.fit_transform(X_train.reshape(X_train.shape[0]*X_train.shape[1], X_train.shape[-1])).reshape(X_train.shape) \n",
    "# doing same for test set\n",
    "X_test = preprocessor.transform(X_test.reshape(X_train.shape[0]*X_train.shape[1], X_test.shape[-1])).reshape(X_test.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Fitting and Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, LSTM, Dense, TimeDistributed\n",
    "\n",
    "\n",
    "# may need to turn this into a function so we can use in hyperparameter tuning\n",
    "\n",
    "\n",
    "## Defining Model\n",
    "\n",
    "# need to separate the stable features from the dynamic features fix this once we know which features\n",
    "X_train_dynamic = None\n",
    "X_train_stable = None\n",
    "\n",
    "\n",
    "# Input for dynamic features\n",
    "dynamic_input = Input(shape=(X_train_dynamic.shape[1], X_train_dynamic.shape[2]))\n",
    "x = LSTM(units=50, return_sequences=True)(dynamic_input) # the parantheses here are telling the function to apply only to the dynamic features\n",
    "\n",
    "\n",
    "# Input for stable features\n",
    "stable_input = Input(shape=(X_train_stable.shape[1],))\n",
    "stable_repeated = Dense(X_train_dynamic.shape[1])(stable_input)\n",
    "stable_repeated = RepeatVector(X_train_dynamic.shape[1])(stable_repeated)\n",
    "stable_repeated = TimeDistributed(Dense(50))(stable_repeated)\n",
    "\n",
    "\n",
    "# Concatenate LSTM output with stable features\n",
    "x = Concatenate()([x, stable_input])\n",
    "\n",
    "# TimeDistributed Dense layer to get prediction for each timestep\n",
    "output = TimeDistributed(Dense(1, activation='sigmoid'))(x)\n",
    "\n",
    "# Define the model\n",
    "model = Model(inputs=[dynamic_input, stable_input], outputs=output)\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics = ['accuracy', 'precision']) \n",
    "# should look as precision because we care more about correctly catching attrition over falsely claiming someone will attrit\n",
    "\n",
    "# Train the model\n",
    "model.fit([X_train_dynamic, X_train_stable], y_train_scaled, epochs=10, validation_split=0.2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## placeholder\n",
    "\n",
    "## we need to see how long it takes to fit one epoch, then do some math to see how many sets of hyperparameters we can test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## placeholder"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
